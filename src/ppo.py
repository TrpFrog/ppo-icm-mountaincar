"""
Policy Proximal Optimization (PPO)

References
  - https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO.py
  - https://tech.morikatron.ai/entry/2020/06/29/100000
"""

from typing import Callable, Optional

import torch
from torch import nn, Tensor
from dataclasses import dataclass
import numpy as np
import tqdm

from src import utils
from src.actor_critic import (
    PPOActorCritic,
    DiscretePPOActorCritic,
    ContinuousPPOActorCritic,
)
from src.icm import Curiosity, NoCuriosity


class RolloutBuffer:
    def __init__(self):
        self.actions = []
        self.states = []
        self.next_states = []
        self.log_probs = []
        self.rewards = []
        self.rewards_with_curiosity = []
        self.is_terminals = []

    def clear(self):
        del self.actions[:]
        del self.states[:]
        del self.next_states[:]
        del self.log_probs[:]
        del self.rewards[:]
        del self.rewards_with_curiosity[:]
        del self.is_terminals[:]


class PPOLoss(nn.Module):
    def __init__(self,
                 eps_clip=0.2,
                 c_entropy=0.01,
                 c_value=0.5,
                 reduction: str = 'mean'):
        super().__init__()
        self.mse_loss = nn.MSELoss(reduction=reduction)
        self.reduction = reduction
        self.c_entropy = c_entropy
        self.c_value = c_value
        assert 0 <= eps_clip < 1
        self.eps_clip = eps_clip

    def clip_loss(self, ratio: Tensor, advantage: Tensor) -> Tensor:
        clipped_advantage = torch.clamp(ratio,
                                        1 - self.eps_clip,
                                        1 + self.eps_clip) * advantage
        unclipped_advantage = ratio * advantage
        return -torch.min(unclipped_advantage, clipped_advantage)

    def state_value_loss(self, state_value: Tensor, reward: Tensor) -> Tensor:
        return self.c_value * self.mse_loss(state_value, reward)

    def entropy_term(self, dist_entropy: Tensor) -> Tensor:
        return -self.c_entropy * dist_entropy

    @staticmethod
    def _flatten_inputs(*x: Tensor) -> map:
        return map(lambda t: t.flatten(), x)

    def forward(self,
                old_log_prob: Tensor,  # (batch_size,)
                log_prob: Tensor,  # (batch_size,) log_prob of action generated by actor
                discounted_reward_sum: Tensor,  # (batch_size,)
                state_value: Tensor,  # (batch_size,) output of critic
                dist_entropy: Tensor,  # (batch_size,)
                ) -> Tensor:
        """
        :param old_log_prob: Log prob. of previous action
        :param log_prob: Log prob. of current action
        :param discounted_reward_sum:
        :param state_value:
        :param dist_entropy: entropy of the policy distribution
        :return:
        """

        (
            log_prob,
            old_log_prob,
            discounted_reward_sum,
            state_value,
            dist_entropy,
        ) = PPOLoss._flatten_inputs(
            log_prob,
            old_log_prob.detach(),
            discounted_reward_sum.detach(),
            state_value,
            dist_entropy,
        )

        assert log_prob.shape == old_log_prob.shape, \
            f'({log_prob.shape = }) != ({old_log_prob.shape = })'
        assert discounted_reward_sum.shape == state_value.shape, \
            f'({discounted_reward_sum.shape = }) != ({state_value.shape = })'

        L_reward = self.state_value_loss(state_value, discounted_reward_sum)

        L_clip = self.clip_loss(
            ratio=torch.exp(log_prob - old_log_prob),
            advantage=discounted_reward_sum - state_value.detach(),
        )

        # PPO is on-policy type algorithm.
        # The following entropy term is added to this loss function
        # to control the trade-off between exploration and exploitation.
        L_entropy = self.entropy_term(dist_entropy)

        return (L_clip + L_reward + L_entropy).mean()  # batch-wise mean


class PPO(nn.Module):

    PPOLossType = Optional[Callable[[Tensor, Tensor, Tensor, Tensor, Tensor], Tensor]]

    @dataclass(frozen=True, kw_only=True)
    class Params:
        gamma_discount: float
        k_epochs: int
        eps_clip: float
        lr_actor: Optional[float] = None
        lr_critic: Optional[float] = None
        lr_curiosity: Optional[float] = None
        reward_normalization: bool = False
        reward_scaling: bool = False
        evaluation_batch_size: Optional[int] = None

    def __init__(self,
                 params: Params,
                 policy: PPOActorCritic,
                 device: torch.device | str = utils.get_device(),
                 curiosity: Optional[Curiosity] = None,
                 custom_optimizer: Optional[torch.optim.Optimizer] = None):

        super().__init__()

        self.params = params
        self.buffer = RolloutBuffer()
        self.device = device
        self.curiosity = curiosity or NoCuriosity()
        self.ppo_loss = PPOLoss(eps_clip=params.eps_clip)
        self._policy = policy

        if custom_optimizer is None:
            assert params.lr_actor is not None, \
                "Params.lr_actor must be specified if custom_optimizer is not specified"
            assert params.lr_critic is not None, \
                "Params.lr_critic must be specified if custom_optimizer is not specified"

            print("using default optimizer")
            print("please check about all parameters are set correctly")

            self.optimizer = torch.optim.Adam([
                {'params': self._policy.actor.parameters(), 'lr': params.lr_actor},
                {'params': self._policy.critic.parameters(), 'lr': params.lr_critic},
                {'params': self.curiosity.parameters(), 'lr': params.lr_curiosity or params.lr_actor},
            ])
        else:
            self.optimizer = custom_optimizer

    @property
    def policy(self):
        if isinstance(self._policy, torch.nn.DataParallel):
            return self._policy.module
        else:
            return self._policy

    @property
    def policy_old(self):
        return self.policy

    def select_action(self, state):
        raise NotImplementedError

    def record_step(self, reward, next_state, is_terminal: bool):
        self.buffer.rewards.append(reward)
        self.buffer.next_states.append(next_state)
        self.buffer.is_terminals.append(is_terminal)

        curiosity_reward = self.curiosity.reward(
            state=self.buffer.states[-1],
            action=self.buffer.actions[-1],
            next_state=next_state
        ).item()

        self.buffer.rewards_with_curiosity.append(reward + (curiosity_reward or 0.0))

    def batchify_state_memory(self, states: list) -> Tensor:
        stacked = torch.stack(states, dim=0)
        while stacked.size(0) <= 1 and stacked.dim() > 2:  # (batch_size, dim)
            stacked = stacked.squeeze(0)
        return stacked

    def batchify_action_memory(self, actions: list):
        stacked = torch.stack(actions, dim=0)
        while stacked.size(0) <= 1 and stacked.dim() > 2:  # (batch_size, dim)
            stacked = stacked.squeeze(0)
        return stacked

    def batchify_log_probs(self, log_probs: list):
        stacked = torch.stack(log_probs, dim=0)
        while stacked.size(0) <= 1 and stacked.dim() > 2:  # (batch_size, dim)
            stacked = stacked.squeeze(0)
        return stacked

    def to(self, device: torch.device | str):
        super().to(device)
        self.device = device
        self.policy.to(device)
        self.policy.device = device
        return self

    def update(self):
        # Monte Carlo estimate of returns
        discounted_rewards = []
        current_discounted_reward = 0

        # Calculate discounted reward
        for reward, is_terminal in zip(reversed(self.buffer.rewards_with_curiosity),
                                       reversed(self.buffer.is_terminals)):
            if is_terminal:
                current_discounted_reward = reward
            else:
                current_discounted_reward = reward + (self.params.gamma_discount * current_discounted_reward)
            discounted_rewards.append(current_discounted_reward)

        # Fix the order of discounted_rewards
        discounted_rewards.reverse()

        # Convert discounted_rewards to tensor
        discounted_rewards = torch.tensor(discounted_rewards).float().to(self.device)

        # Normalizing the discounted_rewards
        if self.params.reward_normalization:
            discounted_rewards = (lambda x: (x - x.mean()) / (x.std() + 1e-7))(discounted_rewards)
        elif self.params.reward_scaling:
            discounted_rewards = (lambda x: x / (x.std() + 1e-7))(discounted_rewards)

        def detach_if_tensor(x):
            if isinstance(x, Tensor):
                return x.detach().to(self.device)
            else:
                return x

        # convert list to tensor
        old_states = detach_if_tensor(self.batchify_state_memory(self.buffer.states))
        old_next_states = detach_if_tensor(self.batchify_state_memory(self.buffer.next_states))
        old_actions = detach_if_tensor(self.batchify_action_memory(self.buffer.actions))
        old_log_probs = detach_if_tensor(self.batchify_log_probs(self.buffer.log_probs))

        # Batchify the data
        bs = self.params.evaluation_batch_size or old_actions.size(0)
        batched = utils.split_to_batches(
            old_states,
            old_next_states,  # next states
            old_actions,
            discounted_rewards.flatten(),
            old_log_probs,
            batch_size=bs
        )

        ppo_losses = []
        curiosity_losses = []
        curiosity_reward = []
        total_reward = []

        # Optimize policy for K epochs
        for epoch in tqdm.trange(self.params.k_epochs, desc="Optimizing policy", leave=False):
            epoch_desc = f"Epoch {epoch}/{self.params.k_epochs}"
            tqdm_iter = tqdm.tqdm(zip(*batched), total=len(batched[0]), desc=epoch_desc)
            for s_old, s_next_old, a_old, acc_reward, log_p_old in tqdm_iter:
                evaluated = self.policy.evaluate(s_old, a_old)

                # Calculate PPO loss
                ppo_loss = self.ppo_loss(
                    old_log_prob=log_p_old,
                    log_prob=evaluated.action_log_probs,
                    discounted_reward_sum=acc_reward.flatten(),
                    state_value=evaluated.state_values.flatten(),
                    dist_entropy=evaluated.dist_entropy,
                )

                # Calculate curiosity loss and add it to the PPO loss
                loss = self.curiosity.loss(
                    policy_loss=ppo_loss,
                    state=s_old,
                    next_state=s_next_old,
                    action=a_old,
                )

                # Optimize the policy
                self.optimizer.zero_grad(set_to_none=True)
                loss.backward()
                self.optimizer.step()

                # Record loss for debugging
                ppo_losses.append(ppo_loss.item())
                if hasattr(self.curiosity, 'latest_info'):
                    curiosity_losses.append(self.curiosity.latest_info['loss'])
                total_reward.append(acc_reward.mean().item())

        # clear buffer
        self.buffer.clear()

        return {
            "ppo_loss": np.mean(ppo_losses),
            "curiosity_loss": np.mean(curiosity_losses) if len(curiosity_losses) > 0 else 0,
            "curiosity_reward": np.mean(curiosity_reward) if len(curiosity_reward) > 0 else 0,
            "total_reward": np.mean(total_reward),
        }


class ContinuousPPO(PPO):

    @dataclass(frozen=True, kw_only=True)
    class Params(PPO.Params):
        action_dim: int
        state_dim: int
        action_std_init: float = 0.6

    def __init__(self,
                 params: Params,
                 custom_policy: Optional[PPOActorCritic] = None,
                 custom_optimizer: Optional[torch.optim.Optimizer] = None):

        if custom_policy is None:
            ac_params = PPOActorCritic.Params(
                state_dim=params.state_dim,
                action_dim=params.action_dim,
                action_std_init=params.action_std_init
            )
            custom_policy = ContinuousPPOActorCritic(params=ac_params)

        super().__init__(params=params,
                         policy=custom_policy,
                         custom_optimizer=custom_optimizer)
        self.action_std = params.action_std_init

    def set_action_std(self, new_action_std):
        """
        Set the action_std of the actor network
        :param new_action_std:
        """
        self.action_std = new_action_std
        assert isinstance(self.policy, ContinuousPPOActorCritic)
        self.policy.set_action_std(new_action_std)
        self.policy_old.set_action_std(new_action_std)

    def decay_action_std(self, action_std_decay_rate, min_action_std):
        """
        Decay the action_std of the actor network
        :param action_std_decay_rate:
        :param min_action_std:
        :return:
        """
        self.action_std = self.action_std - action_std_decay_rate
        self.action_std = round(self.action_std, 4)
        if self.action_std <= min_action_std:
            self.action_std = min_action_std
            print("setting actor output action_std to min_action_std : ", self.action_std)
        else:
            print("setting actor output action_std to : ", self.action_std)
        self.set_action_std(self.action_std)

    def select_action(self, state) -> float:
        with torch.inference_mode():
            state = torch.FloatTensor(state).to(self.device)
            selected_action = self.policy_old.act(state)

        self.buffer.states.append(state)
        self.buffer.actions.append(selected_action.action)
        self.buffer.log_probs.append(selected_action.log_prob)

        return selected_action.action.detach().cpu().numpy().flatten()


class DiscretePPO(PPO):

    @dataclass(frozen=True, kw_only=True)
    class Params(PPO.Params):
        action_dim: int
        state_dim: int

    def __init__(self,
                 params: Params,
                 custom_policy: Optional[PPOActorCritic] = None,
                 custom_optimizer: Optional[torch.optim.Optimizer] = None,
                 curiosity: Optional[Curiosity] = None):

        if custom_policy is None:
            ac_params = PPOActorCritic.Params(
                state_dim=params.state_dim,
                action_dim=params.action_dim,
                action_std_init=0  # not used
            )
            custom_policy = DiscretePPOActorCritic(params=ac_params)

        super().__init__(params=params,
                         policy=custom_policy,
                         custom_optimizer=custom_optimizer,
                         curiosity=curiosity)

    def select_action(self, state: Tensor, greedy: bool = False) -> int:
        with torch.inference_mode():
            selected_action = self.policy.act(state.unsqueeze(0), greedy=greedy)

        self.buffer.states.append(state)
        self.buffer.actions.append(selected_action.action)
        self.buffer.log_probs.append(selected_action.log_prob)

        return selected_action.action.item()
